{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q pandas transformers tensorflow-hub faiss-gpu annoy torch torchvision elasticsearch elasticsearch-dsl seaborn\n",
    "!pip install -q -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch_dsl import Search, Q, SF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead, modeling_utils\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.notebook import tqdm_notebook as tqdmnb\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_USE = 'bert-large-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_TO_USE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TO_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "args = pd.read_pickle(f'args_encoded_{MODEL_TO_USE}.pkl')\n",
    "args.dropna(inplace=True)\n",
    "args.reset_index(inplace=True, drop=True)\n",
    "\n",
    "arg_representations = np.load(f'arg_representations_{MODEL_TO_USE}.npy')\n",
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalized_representation = normalizer.fit_transform(arg_representations) \n",
    "\n",
    "dataset = pd.read_pickle('dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized = []\n",
    "\n",
    "for chunk in tqdmnb(np.array_split(dataset, 5), total=5):\n",
    "    tokenized_chunk = tokenizer.batch_encode_plus(list(chunk['text'].values), max_length=tokenizer.max_len, pad_to_max_length=True, return_overflowing_tokens=True)\n",
    "    tokenized_chunk.pop('token_type_ids')\n",
    "    \n",
    "    overflow_index = tokenized_chunk.pop('overflow_to_sample_mapping')\n",
    "    \n",
    "    # Repeating indices are included as lists of the corresponding index\n",
    "    overflow_index = np.hstack(overflow_index)\n",
    "    text_ids = chunk['id'].values\n",
    "    text_ids = text_ids[overflow_index]\n",
    "    \n",
    "    df = pd.DataFrame(tokenized_chunk)\n",
    "    df['id'] = text_ids\n",
    "    tokenized.append(df)\n",
    "tokenized = pd.concat(tokenized)\n",
    "tokenized.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized['input_ids'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test[np.hstack(overflow_map[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = normalized_representation.shape[1]\n",
    "m=64\n",
    "n_bits=8\n",
    "nlist = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a FAISS PQ-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pq = faiss.IndexPQ(d, m, n_bits)\n",
    "pq.train(normalized_representation)\n",
    "pq.add(normalized_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a FAISS dot-product IVF-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "quantizer = faiss.IndexFlatIP(d)\n",
    "index_dp = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "index_dp.train(normalized_representation)\n",
    "index_dp.add(normalized_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a FAISS IVF/PQ-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = 256                             \n",
    "quantizer = faiss.IndexFlatL2(d)  # this remains the same\n",
    "index_ivfpq = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "index_ivfpq.train(arg_representations)\n",
    "index_ivfpq.add(arg_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_faiss, I_faiss = index_ivfpq.search(normalized_representation[:1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Annoy index with 1000 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f = 1024\n",
    "annoy = AnnoyIndex(f, 'angular')\n",
    "for i, arg in tqdmnb(enumerate(normalized_representation), total=normalized_representation.shape[0]):\n",
    "    annoy.add_item(i, arg)\n",
    "print(\"Now building Index...\")\n",
    "annoy.build(1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy.save(f'{MODEL_TO_USE}_1000_trees_angular.ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annoy index from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u= AnnoyIndex(f,'angular')\n",
    "u.load(f'{MODEL_TO_USE}_100_trees_angular.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = indexl2.search(np.expand_dims(query, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(arg_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Computer Science is a bad university degree.\"\n",
    "THRESHOLD_ANNOY = 0.8\n",
    "THRESHOLD_FAISS = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "tokenized_query = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(query))\n",
    "encoded_query = tokenizer.prepare_for_model(tokenized_query, max_length=512, add_special_tokens=True, pad_to_max_length=True)\n",
    "\n",
    "inp, mask = encoded_query['input_ids'], encoded_query['attention_mask']\n",
    "inp, mask = torch.tensor(inp).unsqueeze(0), torch.tensor(mask).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_rep = model(inp, attention_mask=mask)[0][:,0,:].numpy()\n",
    "query_rep_normalized = normalizer.transform(query_rep)\n",
    "\n",
    "D_faiss, I_faiss = pq.search(query_rep_normalized, 100)   \n",
    "I_annoy, D_annoy = u.get_nns_by_vector(query_rep_normalized.squeeze(), 100, search_k=-1, include_distances=True)\n",
    "I_faiss = I_faiss[I_faiss > -1]\n",
    "\n",
    "# #Annoy returns angular distance, not cosine similarity\n",
    "cos_sim_annoy = 1 - np.square(D_annoy)/2\n",
    "cos_sim_faiss = 1 - np.square(D_faiss)/2\n",
    "\n",
    "\n",
    "arg_ids_annoy = set(args['id'][I_annoy].values)\n",
    "arg_ids_faiss = set(args['id'][list(I_faiss.squeeze())].values)\n",
    "\n",
    "intersection = list(arg_ids_annoy.intersection(arg_ids_faiss))\n",
    "annoy_matches = {arg_id: d for (arg_id, d) in zip(list(arg_ids_annoy), cos_sim_annoy)}\n",
    "faiss_matches = {arg_id: d for (arg_id, d) in zip(list(arg_ids_faiss), list(cos_sim_faiss.squeeze()))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_annoy = {k:v for k,v in annoy_matches.items() if v>THRESHOLD_ANNOY}\n",
    "best_faiss = {k:v for k,v in faiss_matches.items() if v>THRESHOLD_FAISS}\n",
    "\n",
    "best_annoy_weights = {k:len(best_annoy)**(v-0.5) for k,v in annoy_matches.items()}\n",
    "best_faiss_weights = {k:len(best_faiss)**(v-0.7) for k,v in faiss_matches.items()}\n",
    "\n",
    "faiss_functions = [SF({'weight': weight, 'filter': Q('term', _id=arg_id)}) for arg_id, weight in best_faiss_weights.items()]\n",
    "annoy_functions = [SF({'weight': weight, 'filter': Q('term', _id=arg_id)}) for arg_id, weight in best_annoy_weights.items()]\n",
    "functions = faiss_functions + annoy_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# s = Search(using=es, index=\"arg_index\").query(\"match\", text=\"abortion\")\n",
    "# response = s.execute()\n",
    "\n",
    "s = Search(using=es, index=\"arg_index\")   \n",
    "q = Q(\"match\", text=query)  | Q(\"terms\", _id=list(best_annoy.keys()), boost=0.5) | Q(\"terms\", _id=list(best_faiss.keys()), boost=0.4)\n",
    "scored_query = Q('function_score', query=q,functions=functions)\n",
    "s.query = scored_query \n",
    "s = s[:25]\n",
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in response:\n",
    "    print(hit.conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['id'].isin(best_annoy)]['context.discussionTitle'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset[dataset['id'].isin(list(arg_ids_annoy))]['context.discussionTitle'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset[dataset['id'].isin(list(arg_ids_faiss))]['context.discussionTitle'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['id'].isin(intersection)]['context.discussionTitle'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['127.0.0.1:9200/'], verify_certs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with BERT MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= \"Hello my name is [MASK] and I am here to [MASK] you to the [MASK] of [MASK].\"\n",
    "inp_tens = torch.tensor(tokenizer.encode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(inp)))).unsqueeze(0)\n",
    "mask_indices = np.nonzero(inp_tens.squeeze()==103).squeeze()\n",
    "preds = model(inp_tens)[0].squeeze()\n",
    "for i in list(mask_indices):\n",
    "    hallucinated = inp_tens.squeeze()\n",
    "    hallucinated[i] = torch.argmax(preds[i]).item()\n",
    "tokenizer.decode(hallucinated, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'Once upon a midnight dreary while I pondered weak and [MASK].'\n",
    "inp_tens = torch.tensor(tokenizer.encode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(inp)))).unsqueeze(0)\n",
    "mask_indices = np.nonzero(inp_tens.squeeze()==103).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_context = 'cow milk good because'\n",
    "input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n",
    "outputs = model.generate(max_length=30, input_ids=input_ids, do_sample=True, num_beams=10, top_k=100 , top_p=0.1, num_return_sequences=5, temperature=0.8, repetition_penalty=20)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
    "for i in range(5): #  3 output sequences were generated\n",
    "    print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_context = 'cow milk bad because'\n",
    "input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n",
    "outputs = model.generate(max_length=30, input_ids=input_ids, do_sample=True, num_beams=5, top_k=100 , top_p=0.9, num_return_sequences=5, temperature=1.2, repetition_penalty=20)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
    "for i in range(3): #  3 output sequences were generated\n",
    "    print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2000)\n",
    "x = vectorizer.fit(dataset['text'].values)\n",
    "bow = x.transform(dataset['text'].values)\n",
    "ocurrences = bow.sum(axis=0)\n",
    "for word, i in x.vocabulary_.items():\n",
    "    cnt[word] += ocurrences[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt.most_common(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments = pd.read_csv('./Data/arguments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments['Premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "x = vectorizer.fit(judgments['Premise'].values)\n",
    "bow = x.transform(judgments['Premise'].values)\n",
    "ocurrences = bow.sum(axis=0)\n",
    "for word, i in x.vocabulary_.items():\n",
    "    cnt[word] += ocurrences[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(judgments[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
