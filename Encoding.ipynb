{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --disable-pip-version-check -q pandas transformers seaborn tensorflow_hub elasticsearch elasticsearch-dsl annoy faiss-gpu\n",
    "!pip install --disable-pip-version-check -Uq scikit-learn\n",
    "!pip -q install --disable-pip-version-check --no-warn-script-location --user tensorflow-text \n",
    "!pip -q uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_USE = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the JSON file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled dataset already exists. Now loading dataset.pkl into Pandas DataFrame\n",
      " \n",
      "CPU times: user 673 ms, sys: 640 ms, total: 1.31 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_directory = Path('.')\n",
    "if not (current_directory / 'Data/dataset.pkl').exists():\n",
    "    print(\"Pickled dataset doesn't already exists. Now reading JSON file.\")\n",
    "\n",
    "    #Read in JSON file if pickled dataframe doesn't already exist\n",
    "    with open('./Data/args-me.json') as f:\n",
    "        d = json.load(f)\n",
    "        d = d['arguments']\n",
    "        context_subfields = [['context', k] for k in d[0]['context'].keys()]\n",
    "        dataset = pd.json_normalize(d, record_path='premises', meta=['id', 'conclusion', *context_subfields])\n",
    "        print(\"Now pickling Pandas DataFrame into dataset.pkl.\")\n",
    "        dataset.to_pickle('Data/dataset.pkl')\n",
    "        print(\"DataFrame pickled.\")\n",
    "        print(\" \")\n",
    "else:\n",
    "    #If pickle already exists, read it into dataframe\n",
    "    print(\"Pickled dataset already exists. Now loading dataset.pkl into Pandas DataFrame\")\n",
    "    dataset = pd.read_pickle('Data/dataset.pkl')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387692, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset, model_to_use, chunks=5, folder_name=\"Tokenized\"):\n",
    "    current_directory = Path('.')\n",
    "    if not (current_directory / f'{folder_name}/{model_to_use}.pkl').exists():\n",
    "        print(\"Pickled dataset doesn't already exists. Now Tokenizing arguments dataframe.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_to_use, use_fast=True)\n",
    "        tokenized = []\n",
    "        for chunk in tqdm_notebook(np.array_split(dataset, chunks), total=chunks):\n",
    "            tokenized_chunk = tokenizer.batch_encode_plus(list(chunk['text'].values), max_length=tokenizer.max_len, pad_to_max_length=True, return_overflowing_tokens=True)\n",
    "            tokenized_chunk.pop('token_type_ids')\n",
    "\n",
    "            overflow_index = tokenized_chunk.pop('overflow_to_sample_mapping')\n",
    "\n",
    "            # Repeating indices are included as lists of the corresponding index eg: [0,1, [2,2,2,2], [3,3]...]\n",
    "            overflow_index = np.hstack(overflow_index)\n",
    "            text_ids = chunk['id'].values\n",
    "            text_ids = text_ids[overflow_index]\n",
    "\n",
    "            df = pd.DataFrame(tokenized_chunk)\n",
    "            df[['input_ids', 'attention_mask']] = df[['input_ids', 'attention_mask']].applymap(np.array)\n",
    "            df['id'] = text_ids\n",
    "            tokenized.append(df)\n",
    "        tokenized = pd.concat(tokenized)\n",
    "        tokenized.reset_index(inplace=True, drop=True)\n",
    "        print(f\"Now Pickling DataFrame to Tokenized/{model_to_use}.pkl\")\n",
    "        tokenized.to_pickle(f'{folder_name}/{model_to_use}.pkl')\n",
    "    else:\n",
    "        print(f\"Pickled tokenized dataset already exists. Now loading {folder_name}/{model_to_use}.pkl into Pandas Dataframe\")\n",
    "        tokenized = pd.read_pickle(f'{folder_name}/{model_to_use}.pkl')\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the Dataset using the pretrained tokenizer of MODEL_TO_USE. If previously tokenized, load the corresponding dataframe from the saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled tokenized dataset already exists. Now loading Tokenized/distilbert-base-uncased.pkl into Pandas Dataframe\n",
      "CPU times: user 3.32 s, sys: 2.48 s, total: 5.8 s\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized = tokenize(dataset, MODEL_TO_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2026, 7116, 2005, 21156, 2098, 2296, 246...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>c67482ba-2019-04-18T13:32:05Z-00000-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2129, 2079, 2017, 16599, 1996, 2082, 209...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>c67482ba-2019-04-18T13:32:05Z-00001-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 2816, 2031, 2053, 17075, 3037, 1999, 434...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>c67482ba-2019-04-18T13:32:05Z-00002-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 2004, 1037, 3026, 2012, 2026, 2082, 1012...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>c67482ba-2019-04-18T13:32:05Z-00003-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 1996, 5813, 2109, 2011, 4013, 1008, 1598...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>4d3d4471-2019-04-18T11:45:01Z-00000-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555578</th>\n",
       "      <td>[101, 6662, 5472, 28212, 4801, 1012, 1000, 212...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>671509c8-2019-04-17T11:47:34Z-00067-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555579</th>\n",
       "      <td>[101, 6111, 2816, 4013, 15509, 9250, 2205, 285...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>671509c8-2019-04-17T11:47:34Z-00052-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555580</th>\n",
       "      <td>[101, 2270, 2816, 2064, 3749, 2673, 6111, 2816...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>671509c8-2019-04-17T11:47:34Z-00037-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555581</th>\n",
       "      <td>[101, 6111, 2816, 2024, 15011, 2797, 2816, 102...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>671509c8-2019-04-17T11:47:34Z-00022-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555582</th>\n",
       "      <td>[101, 6111, 2816, 2024, 18516, 2087, 2011, 213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>671509c8-2019-04-17T11:47:34Z-00007-000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555583 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_ids  \\\n",
       "0       [101, 2026, 7116, 2005, 21156, 2098, 2296, 246...   \n",
       "1       [101, 2129, 2079, 2017, 16599, 1996, 2082, 209...   \n",
       "2       [101, 2816, 2031, 2053, 17075, 3037, 1999, 434...   \n",
       "3       [101, 2004, 1037, 3026, 2012, 2026, 2082, 1012...   \n",
       "4       [101, 1996, 5813, 2109, 2011, 4013, 1008, 1598...   \n",
       "...                                                   ...   \n",
       "555578  [101, 6662, 5472, 28212, 4801, 1012, 1000, 212...   \n",
       "555579  [101, 6111, 2816, 4013, 15509, 9250, 2205, 285...   \n",
       "555580  [101, 2270, 2816, 2064, 3749, 2673, 6111, 2816...   \n",
       "555581  [101, 6111, 2816, 2024, 15011, 2797, 2816, 102...   \n",
       "555582  [101, 6111, 2816, 2024, 18516, 2087, 2011, 213...   \n",
       "\n",
       "                                           attention_mask  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "555578  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "555579  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "555580  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "555581  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "555582  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             id  \n",
       "0       c67482ba-2019-04-18T13:32:05Z-00000-000  \n",
       "1       c67482ba-2019-04-18T13:32:05Z-00001-000  \n",
       "2       c67482ba-2019-04-18T13:32:05Z-00002-000  \n",
       "3       c67482ba-2019-04-18T13:32:05Z-00003-000  \n",
       "4       4d3d4471-2019-04-18T11:45:01Z-00000-000  \n",
       "...                                         ...  \n",
       "555578  671509c8-2019-04-17T11:47:34Z-00067-000  \n",
       "555579  671509c8-2019-04-17T11:47:34Z-00052-000  \n",
       "555580  671509c8-2019-04-17T11:47:34Z-00037-000  \n",
       "555581  671509c8-2019-04-17T11:47:34Z-00022-000  \n",
       "555582  671509c8-2019-04-17T11:47:34Z-00007-000  \n",
       "\n",
       "[555583 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn the 'tokenized' Dataframe columns input_ids and attention_mask into a Tensorflow Dataset to feed into MODEL_TO_USE and encode the arguments into dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenized_df=None, model_to_use=None, batch_size=32, folder_name=\"Encoded\", dtype=np.float16):\n",
    "    current_directory = Path('.')\n",
    "    config = AutoConfig.from_pretrained(MODEL_TO_USE)\n",
    "    config.output_hidden_states=True\n",
    "    encoded = None\n",
    "    if not (current_directory / f'{folder_name}/{model_to_use}.npy').exists():\n",
    "        print(f\"Encodings don't yet exist. Now feeding tokens into {model_to_use}. This will take a while\")\n",
    "        \n",
    "        model = TFAutoModel.from_pretrained(model_to_use, config=config)\n",
    "\n",
    "        i = np.stack(tokenized_df['input_ids'])\n",
    "        m = np.stack(tokenized_df['attention_mask'])\n",
    "\n",
    "        dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(i), tf.data.Dataset.from_tensor_slices(m)))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        encoded = np.zeros((i.shape[0], config.dim*(config.n_layers+1)))\n",
    "        \n",
    "        #Feed tokens in batches of batch_size into the huggingface model\n",
    "        for iteration, (input_tensor, mask_tensor) in tqdm_notebook(enumerate(dataset), total=i.shape[0]/batch_size):\n",
    "            #SET TRAINING TO FALSE TO TURN OFF DROPOUT\n",
    "            output = model(input_tensor, attention_mask=mask_tensor, training=False)\n",
    "            hidden = np.hstack([thing.numpy()[:,0,:] for thing in reversed(output[-1])])\n",
    "            encoded[iteration*batch_size:(iteration+1)*batch_size] = hidden\n",
    "        print(f\"Saving encodings in ./{folder_name}. Now writing ./Encoded/{model_to_use}.npy\")\n",
    "\n",
    "        np.save(f'./Encoded/{model_to_use}.npy', encoded)\n",
    "    if not encoded:\n",
    "        print(f\"Encodings are in ./{folder_name}. Now reading ./Encoded/{model_to_use}.npy\")\n",
    "        encoded = np.load(f'./Encoded/{model_to_use}.npy')\n",
    "\n",
    "        #last part of encoded is the embeddings at the input, so they're all the same: the input embedding for [CLS] before it's fed into the network\n",
    "        encoded = encoded[:, :-config.dim]\n",
    "        encoded = encoded.astype(dtype=dtype, copy=False)\n",
    "\n",
    "        print(f\"Loaded model embeddings are {config.dim}-dimensional, and {model_to_use} has {config.n_layers} hidden layers.\")\n",
    "        print(f\"Using dtype={dtype}\")\n",
    "    return encoded, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings are in ./Encoded. Now reading ./Encoded/distilbert-base-uncased.npy\n",
      "Loaded model embeddings are 768-dimensional, and distilbert-base-uncased has 6 hidden layers.\n",
      "Using dtype=<class 'numpy.float32'>\n",
      "CPU times: user 2.11 s, sys: 24.7 s, total: 26.8 s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoded, config = encode(tokenized_df=tokenized, model_to_use=MODEL_TO_USE, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use an Autoencoder to reduce the dimensionality of the embedding down to 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(model_to_use,test_size=0.15, dtype=np.float32, batch_size=32):\n",
    "    print(f\"Now reading ./Encoded/{model_to_use}.npy...\")\n",
    "    encoded = np.load(f'./Encoded/{model_to_use}.npy')\n",
    "    config = AutoConfig.from_pretrained(MODEL_TO_USE)\n",
    "    #last part of encoded is the embeddings at the input, so they're all the same: the input embedding for [CLS] at before it's fed into the network\n",
    "    encoded = encoded[:, :-config.dim]\n",
    "    encoded = encoded.astype(dtype=dtype, copy=False)\n",
    "    \n",
    "    X_train, X_valid = train_test_split(encoded, test_size=0.15)\n",
    "    print(\"Now creating TensorFlow Dataset\")\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "    dataset_valid = tf.data.Dataset.from_tensor_slices(X_valid)\n",
    "    \n",
    "    dataset_train = dataset_train.map(lambda x: (x,x))\n",
    "    dataset_train = dataset_train.shuffle(10000)\n",
    "    dataset_train = dataset_train.batch(batch_size)\n",
    "    \n",
    "    dataset_valid = dataset_valid.map(lambda x: (x,x))\n",
    "    dataset_valid = dataset_valid.shuffle(10000)\n",
    "    dataset_valid = dataset_valid.batch(batch_size)\n",
    "    \n",
    "    return dataset_train, dataset_valid\n",
    "\n",
    "#From Aurelien Geron's Hands-on Machine Learning 2nd ed. https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense=dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\", shape=[self.dense.input_shape[-1]])\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now reading ./Encoded/distilbert-base-uncased.npy...\n",
      "Now creating TensorFlow Dataset\n"
     ]
    }
   ],
   "source": [
    "d_train, d_valid = make_dataset(model_to_use=MODEL_TO_USE, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 14758 steps, validate for 2605 steps\n",
      "Epoch 1/10\n",
      "14758/14758 [==============================] - 140s 10ms/step - loss: 0.3169 - accuracy: 0.9703 - val_loss: 0.2989 - val_accuracy: 0.9990\n",
      "Epoch 2/10\n",
      "14758/14758 [==============================] - 135s 9ms/step - loss: 0.2982 - accuracy: 0.9990 - val_loss: 0.2978 - val_accuracy: 0.9990\n",
      "Epoch 3/10\n",
      "14758/14758 [==============================] - 137s 9ms/step - loss: 0.2975 - accuracy: 0.9990 - val_loss: 0.2972 - val_accuracy: 0.9990\n",
      "Epoch 4/10\n",
      "14758/14758 [==============================] - 137s 9ms/step - loss: 0.2970 - accuracy: 0.9990 - val_loss: 0.2968 - val_accuracy: 0.9990\n",
      "Epoch 5/10\n",
      "14758/14758 [==============================] - 137s 9ms/step - loss: 0.2965 - accuracy: 0.9990 - val_loss: 0.2962 - val_accuracy: 0.9990\n",
      "Epoch 6/10\n",
      "14758/14758 [==============================] - 140s 9ms/step - loss: 0.2960 - accuracy: 0.9990 - val_loss: 0.2958 - val_accuracy: 0.9990\n",
      "Epoch 7/10\n",
      "14758/14758 [==============================] - 137s 9ms/step - loss: 0.2957 - accuracy: 0.9990 - val_loss: 0.2956 - val_accuracy: 0.9990\n",
      "Epoch 8/10\n",
      "14758/14758 [==============================] - 136s 9ms/step - loss: 0.2954 - accuracy: 0.9990 - val_loss: 0.2952 - val_accuracy: 0.9990\n",
      "Epoch 9/10\n",
      "14758/14758 [==============================] - 137s 9ms/step - loss: 0.2950 - accuracy: 0.9990 - val_loss: 0.2949 - val_accuracy: 0.9990\n",
      "Epoch 10/10\n",
      "14758/14758 [==============================] - 143s 10ms/step - loss: 0.2948 - accuracy: 0.9990 - val_loss: 0.2948 - val_accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "dense_1 = keras.layers.Dense(3072, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(2048, activation=\"selu\")\n",
    "dense_3 = keras.layers.Dense(1024, activation=\"selu\")\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=d_train.element_spec[0].shape[1:]),\n",
    "    dense_1,\n",
    "    dense_2,\n",
    "    dense_3\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_3, activation=\"selu\"),\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "tied_ae = keras.models.Sequential([encoder, tied_decoder])\n",
    "\n",
    "callback_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "\n",
    "tied_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adagrad(), metrics=[\"accuracy\"])\n",
    "\n",
    "hist = tied_ae.fit(d_train, epochs=10, validation_data=d_valid, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Autoencoder_encoder/Encoderdistilbert-base-uncased_1024/0001')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = \"0001\"\n",
    "model_name = f\"Encoder{MODEL_TO_USE}_1024\"\n",
    "folder_name = \"Autoencoder_encoder\"\n",
    "model_path = Path('.') / folder_name / model_name / model_version\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Autoencoder_encoder/Encoderdistilbert-base-uncased_1024/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Autoencoder_encoder/Encoderdistilbert-base-uncased_1024/0001/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(encoder, str(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved = tf.saved_model.load(str(model_path))\n",
    "tf.reduce_all(saved(encoded[:1], training=False) == encoder.predict(encoded[:1])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.predict(encoded[:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b552ba5e5ce1424e91c0adebfdddd6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1086.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ae_e = tf.saved_model.load(str(model_path))\n",
    "autoencoded = []\n",
    "to_autoencode = tf.data.Dataset.from_tensor_slices(encoded)\n",
    "to_autoencode = to_autoencode.batch(512)\n",
    "\n",
    "for batch in tqdm_notebook(to_autoencode, total=tf.data.experimental.cardinality(to_autoencode).numpy()):\n",
    "    autoencoded.append(ae_e(batch, training=False).numpy())\n",
    "autoencoded = np.vstack(autoencoded)\n",
    "np.save(f\"./Encoded/autoencoded_{MODEL_TO_USE}_1024.npy\", autoencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(555583, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Google's Universal Sentence Encoder to  encode the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 170.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 350.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 530.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 710.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 890.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[ 0.02881766, -0.02020015,  0.01069627, ..., -0.02896921,\n",
       "         0.00876467,  0.08242127],\n",
       "       [ 0.04333361, -0.01821983,  0.01752458, ..., -0.02157544,\n",
       "        -0.02861957,  0.05987324]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed([\"This is a sentence\", \"This is another sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "to_embed = tf.data.Dataset.from_tensor_slices(dataset['text'].values)\n",
    "to_embed = to_embed.batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8e7b47be294b0d91f9b4a1fc6920ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm_notebook(to_embed, total=tf.data.experimental.cardinality(to_embed).numpy()):\n",
    "    embedding = embed(batch)\n",
    "    embedding = embedding.numpy()\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Encoded/UniversalSentenceEncoderEmbeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
